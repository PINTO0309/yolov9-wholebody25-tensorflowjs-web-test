<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>TensorFlow.js リアルタイム物体検出</title>
  <style>
    canvas {
      border: 1px solid black;
    }
  </style>
</head>
<body>

  <!-- カメラ映像を表示するための video 要素 -->
  <video id="video" width="640" height="480" autoplay></video>

  <!-- 推論結果を描画するための canvas -->
  <canvas id="output-canvas"></canvas>

  <!-- 推論時間を表示するための要素 -->
  <div id="inference-time" style="margin-top: 10px;"></div>

  <!-- TensorFlow.js を CDN から読み込む -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>

  <script>
    const VIDEO_INPUT_WIDTH = 640;   // ビデオの入力幅
    const VIDEO_INPUT_HEIGHT = 480;  // ビデオの入力高
    const MODEL_INPUT_WIDTH = 320;   // モデルの入力幅
    const MODEL_INPUT_HEIGHT = 256;  // モデルの入力高

    const classColors = [
      "#FF0000", "#00FF00", "#0000FF", "#FFFF00", "#FF00FF", "#00FFFF", "#FFA500", "#800080",
      "#008080", "#A52A2A", "#808000", "#000080", "#FFC0CB", "#800000", "#808080", "#C0C0C0",
      "#FFD700", "#ADFF2F", "#87CEEB", "#DC143C", "#FF4500", "#2E8B57", "#4682B4", "#6A5ACD",
      "#708090"
    ];

    let model; // モデルをロードして保持する変数

    // カメラ映像を取得する関数
    async function setupCamera() {
      const video = document.getElementById('video');
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { width: VIDEO_INPUT_WIDTH, height: VIDEO_INPUT_HEIGHT }
      });
      video.srcObject = stream;

      return new Promise((resolve) => {
        video.onloadedmetadata = () => {
          resolve(video);
        };
      });
    }

    // TensorFlow.js モデルのロード
    async function loadModel() {
      try {
        model = await tf.loadGraphModel('tfjs_model_low/model.json'); // 適切なパスに変更
        return model;
      } catch (err) {
        console.error('モデルのロードに失敗しました:', err);
        alert('モデルのロードに失敗しました。');
      }
    }

    // フレームを TensorFlow.js 用に前処理する関数
    function preprocessImage(videoElement) {
      let tensor = tf.browser.fromPixels(videoElement).expandDims(0).toFloat();
      // RGB を BGR に変換
      tensor = tf.reverse(tensor, axis=[-1]); // 最後の次元 (チャネル) を反転
      return tensor;
    }

    // 推論の実行と時間計測
    async function runInference(inputTensor) {
      try {
        const startTime = Date.now();
        const output = await model.executeAsync(inputTensor);
        const endTime = Date.now();
        const inferenceTime = endTime - startTime;
        document.getElementById('inference-time').textContent = `推論時間: ${inferenceTime} ミリ秒`;
        return output;
      } catch (err) {
        console.error('推論に失敗しました:', err);
        alert('推論に失敗しました。');
      }
    }

    const threshold = 0.5;

    // 推論結果に基づいてバウンディングボックスを描画
    function renderBoundingBoxes(output, videoElement) {
      const canvas = document.getElementById('output-canvas');
      const width = videoElement.videoWidth;
      const height = videoElement.videoHeight;
      canvas.width = width;
      canvas.height = height;

      const ctx = canvas.getContext('2d');
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);

      // 元のconsole.warn()を保存
      const originalWarn = console.warn;

      // 特定のワーニングメッセージを抑制
      console.warn = (message) => {
        if (!message.includes("This model execution did not contain any nodes with control flow")) {
          originalWarn(message);
        }
      };

      const boxesData = output.arraySync(); // テンソルのデータを取得

      for (let i = 0; i < boxesData.length; i++) {
        const [batchno, classId, score, x1, y1, x2, y2] = boxesData[i];

        // レンダリング除外クラスID
        const excludedIds = [1, 2, 3, 4, 8, 9, 10, 11, 12, 13, 14, 15, 22, 23];
        if (excludedIds.includes(classId)) {
          continue;
        }

        if (score > threshold) {
          const scaleX = canvas.width / MODEL_INPUT_WIDTH;
          const scaleY = canvas.height / MODEL_INPUT_HEIGHT;

          const boxColor = classColors[classId % classColors.length];

          ctx.strokeStyle = boxColor;
          ctx.lineWidth = 2;
          ctx.strokeRect(x1 * scaleX, y1 * scaleY, (x2 - x1) * scaleX, (y2 - y1) * scaleY);

          ctx.font = '16px Arial';
          ctx.fillStyle = boxColor;
          ctx.fillText(`Class: ${classId} Score: ${score.toFixed(2)}`, x1 * scaleX, y1 * scaleY - 5);
        }
      }
    }

    // リアルタイムでカメラの映像を処理する関数
    async function detectFrame(videoElement) {
      tf.engine().startScope();
      const inputTensor = preprocessImage(videoElement);
      const output = await runInference(inputTensor);
      renderBoundingBoxes(output, videoElement);
      requestAnimationFrame(() => detectFrame(videoElement)); // 次のフレームを処理
      inputTensor.dispose(); // ここでメモリを解放
      output.dispose(); // ここでメモリを解放
      tf.engine().endScope();
    }

    // 初期化関数
    async function init() {
      await setupCamera();
      await loadModel();
      const video = document.getElementById('video');
      detectFrame(video); // フレームごとに検出処理を実行
    }

    // ページ読み込み時に初期化
    window.onload = init;
  </script>

</body>
</html>
