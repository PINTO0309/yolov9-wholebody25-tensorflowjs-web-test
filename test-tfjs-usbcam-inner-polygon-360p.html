<!DOCTYPE html>
<html lang="ja">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>TensorFlow.js リアルタイム物体検出</title>
  <style>
    canvas {
      border: 1px solid black;
    }
    #alert {
      position: absolute;
      top: 10px;
      left: 10px;
      padding: 10px;
      background-color: rgba(255, 0, 0, 0.8);
      color: white;
      font-size: 20px;
      display: none;
    }
  </style>
</head>
<body>

  <div id="alert">Entered the area</div>
  <video id="video" width="640" height="360" autoplay></video>
  <canvas id="output-canvas" width="640" height="360"></canvas>
  <button id="reset-button">Reset Polygons</button>
  <button id="save-json-button">Save Polygons to JSON</button>
  <button id="load-json-button">Load Polygons from JSON</button>
  <div id="inference-time" style="margin-top: 10px;"></div>

  <script src="https://cdn.jsdelivr.net/npm/@tensorflow/tfjs"></script>

  <script>
    const VIDEO_INPUT_WIDTH = 640;
    const VIDEO_INPUT_HEIGHT = 360;
    const MODEL_INPUT_WIDTH = 672;
    const MODEL_INPUT_HEIGHT = 384;

    const classColors = [
      "#FF0000", "#00FF00", "#0000FF", "#FFFF00", "#FF00FF", "#00FFFF", "#FFA500", "#800080",
      "#008080", "#A52A2A", "#808000", "#000080", "#FFC0CB", "#800000", "#808080", "#C0C0C0",
      "#FFD700", "#ADFF2F", "#87CEEB", "#DC143C", "#FF4500", "#2E8B57", "#4682B4", "#6A5ACD",
      "#708090"
    ];

    let model;

    // カメラ映像の取得
    async function setupCamera() {
      const video = document.getElementById('video');
      const stream = await navigator.mediaDevices.getUserMedia({
        video: { width: VIDEO_INPUT_WIDTH, height: VIDEO_INPUT_HEIGHT }
      });
      video.srcObject = stream;
      return new Promise((resolve) => {
        video.onloadedmetadata = () => resolve(video);
      });
    }

    // モデルのロード
    async function loadModel() {
      try {
        model = await tf.loadGraphModel('tfjs_model_384x672/model.json');
        return model;
      } catch (err) {
        console.error('モデルのロードに失敗しました:', err);
        alert('モデルのロードに失敗しました。');
      }
    }

    // 前処理
    function preprocessImage(videoElement) {
      let tensor = tf.browser.fromPixels(videoElement).expandDims(0).toFloat();
      tensor = tf.reverse(tensor, axis=[-1]);
      return tensor;
    }

    // 推論の実行
    async function runInference(inputTensor) {
      const startTime = Date.now();
      const output = await model.executeAsync(inputTensor);
      const endTime = Date.now();
      const inferenceTime = endTime - startTime;
      document.getElementById('inference-time').textContent = `推論時間: ${inferenceTime} ミリ秒`;
      return output;
    }

    const threshold = 0.35;

    let currentPolygon = [];
    let polygons = [];
    let isDrawing = false;

    const canvas = document.getElementById('output-canvas');
    const ctx = canvas.getContext('2d');

    // マウスクリックで頂点を追加
    canvas.addEventListener('click', function (e) {
      const rect = canvas.getBoundingClientRect();
      const x = e.clientX - rect.left;
      const y = e.clientY - rect.top;

      currentPolygon.push([x, y]);
      isDrawing = true;
      drawCurrentPolygon(video);
    });

    // ダブルクリックで多角形を確定
    canvas.addEventListener('dblclick', function (e) {
      if (currentPolygon.length >= 3) {
        polygons.push([...currentPolygon]);  // 多角形を確定
        currentPolygon = [];  // 新しい多角形に向けてリセット
        isDrawing = false;
        drawPolygons(ctx, polygons);  // 確定した多角形を描画
      }
    });

    // リセットボタンで多角形をリセット
    document.getElementById('reset-button').addEventListener('click', function () {
      polygons = [];
      currentPolygon = [];
      ctx.clearRect(0, 0, canvas.width, canvas.height);
    });

    // 現在の多角形の頂点を描画する関数
    function drawCurrentPolygon(videoElement) {
      if (currentPolygon.length === 0) return;

      // 背景のカメラ映像を描画
      ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);

      // 確定した多角形も再描画
      drawPolygons(ctx, polygons);

      ctx.beginPath();
      ctx.moveTo(currentPolygon[0][0], currentPolygon[0][1]);

      for (let i = 1; i < currentPolygon.length; i++) {
        ctx.lineTo(currentPolygon[i][0], currentPolygon[i][1]);
      }

      ctx.strokeStyle = 'blue';
      ctx.setLineDash([5, 5]);  // 点線を設定
      ctx.stroke();
      ctx.setLineDash([]);  // 点線を解除
    }

    // 確定した多角形を描画する関数
    function drawPolygons(ctx, polygons) {
      polygons.forEach(polygon => {
        ctx.beginPath();
        ctx.moveTo(polygon[0][0], polygon[0][1]);

        for (let i = 1; i < polygon.length; i++) {
          ctx.lineTo(polygon[i][0], polygon[i][1]);
        }
        ctx.closePath();
        ctx.strokeStyle = 'red';  // 確定した多角形は赤で描画
        ctx.stroke();
      });
    }

    // JSONに保存するボタン
    document.getElementById('save-json-button').addEventListener('click', function () {
      const dataStr = JSON.stringify(polygons);
      const blob = new Blob([dataStr], { type: 'application/json' });
      const url = URL.createObjectURL(blob);
      const a = document.createElement('a');
      a.href = url;
      a.download = 'polygons.json';
      a.click();
      URL.revokeObjectURL(url);
    });

    // JSONから読み込むボタン
    document.getElementById('load-json-button').addEventListener('click', function () {
      const input = document.createElement('input');
      input.type = 'file';
      input.accept = 'application/json';
      input.addEventListener('change', function (e) {
        const file = e.target.files[0];
        const reader = new FileReader();
        reader.onload = function (event) {
          polygons = JSON.parse(event.target.result);
          ctx.clearRect(0, 0, canvas.width, canvas.height);
          drawPolygons(ctx, polygons);
        };
        reader.readAsText(file);
      });
      input.click();
    });

    let frameCounters = {};  // classId 21 の物体が領域に入ったか追跡するオブジェクト

    function renderBoundingBoxes(output, videoElement) {
      const width = videoElement.videoWidth;
      const height = videoElement.videoHeight;
      canvas.width = width;
      canvas.height = height;

      // Webカメラの映像を描画
      ctx.clearRect(0, 0, canvas.width, canvas.height);
      ctx.drawImage(videoElement, 0, 0, canvas.width, canvas.height);

      // 元のconsole.warn()を保存
      const originalWarn = console.warn;

      // 特定のワーニングメッセージを抑制
      console.warn = (message) => {
        if (!message.includes("This model execution did not contain any nodes with control flow")) {
          originalWarn(message);
        }
      };

      // 確定した多角形を描画
      drawPolygons(ctx, polygons);

      // 現在描画中の多角形を描画
      if (isDrawing) {
        drawCurrentPolygon(videoElement);
      }

      // 推論
      const boxesData = output.arraySync();

      // レンダリング除外クラスID
      const excludedIds = [1, 2, 3, 4, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 22, 23];

      for (let i = 0; i < boxesData.length; i++) {
        const [batchno, classId, score, x1, y1, x2, y2] = boxesData[i];

        // 除外されたクラスIDは処理しない
        if (excludedIds.includes(classId)) {
          continue;
        }

        if (score > threshold) {
          const scaleX = canvas.width / MODEL_INPUT_WIDTH;
          const scaleY = canvas.height / MODEL_INPUT_HEIGHT;

          const bx1 = x1 * scaleX;
          const by1 = y1 * scaleY;
          const bx2 = x2 * scaleX;
          const by2 = y2 * scaleY;

          // バウンディングボックスを描画
          ctx.strokeStyle = classColors[classId % classColors.length];
          ctx.lineWidth = 2;
          ctx.strokeRect(bx1, by1, bx2 - bx1, by2 - by1);

          ctx.font = '16px Arial';
          ctx.fillStyle = classColors[classId % classColors.length];
          ctx.fillText(`Class: ${classId} Score: ${score.toFixed(2)}`, bx1, by1 - 5);

          // classId 21 の物体に対して領域内侵入を判定
          if (classId === 21 && score > threshold) {
            const box = [bx1, by1, bx2, by2];

            for (let j = 0; j < polygons.length; j++) {
              if (!frameCounters[i]) {
                frameCounters[i] = {};  // 物体ごとのカウンタを初期化
              }
              if (!frameCounters[i][j]) {
                frameCounters[i][j] = 0;  // 領域ごとのカウンタを初期化
              }

              if (isBoxInPolygon(box, polygons[j])) {
                frameCounters[i][j] += 1;  // 領域ごとのカウンタを更新
                if (frameCounters[i][j] >= 5) {
                  showAlert();  // 物体が 5 フレーム以上領域内にいる場合、警告を表示
                }
              } else {
                frameCounters[i][j] = 0;  // 領域に侵入していない場合はリセット
              }
            }
          }
        }
      }

      // 古い物体のデータをクリーンアップ
      cleanupFrameCounters(output);
    }

    // バウンディングボックスと多角形の重なり判定 (Ray-Casting アルゴリズム)
    function isBoxInPolygon(box, polygon) {
      const [bx1, by1, bx2, by2] = box;
      const boxPoints = [
        [bx1, by1], [bx2, by1], [bx2, by2], [bx1, by2]
      ];

      return boxPoints.some(point => pointInPolygon(point, polygon));
    }

    // 多角形内の点を判定する関数 (Ray-Casting アルゴリズム)
    function pointInPolygon(point, vs) {
      const x = point[0], y = point[1];
      let inside = false;
      for (let i = 0, j = vs.length - 1; i < vs.length; j = i++) {
        const xi = vs[i][0], yi = vs[i][1];
        const xj = vs[j][0], yj = vs[j][1];
        const intersect = ((yi > y) !== (yj > y)) &&
          (x < (xj - xi) * (y - yi) / (yj - yi) + xi);
        if (intersect) inside = !inside;
      }
      return inside;
    }

    // 「Entered the area」を表示
    function showAlert() {
      const alertBox = document.getElementById('alert');
      alertBox.style.display = 'block';

      setTimeout(() => {
        alertBox.style.display = 'none';
      }, 3000);  // 3秒後に非表示
    }

    // 古い物体のデータをクリーンアップ
    function cleanupFrameCounters(output) {
      const boxesData = output.arraySync();
      const detectedIds = boxesData.map((_, i) => i);  // 現在フレームで検出された物体のインデックスリスト

      // 検出されていない物体のカウンタを削除
      for (let id in frameCounters) {
        if (!detectedIds.includes(parseInt(id))) {
          delete frameCounters[id];
        }
      }
    }

    // フレーム処理
    async function detectFrame(videoElement) {
      tf.engine().startScope();
      const inputTensor = preprocessImage(videoElement);
      const output = await runInference(inputTensor);
      renderBoundingBoxes(output, videoElement);
      requestAnimationFrame(() => detectFrame(videoElement));
      inputTensor.dispose();
      output.dispose();
      tf.engine().endScope();
    }

    // 初期化
    async function init() {
      await setupCamera();
      await loadModel();
      const video = document.getElementById('video');
      detectFrame(video);
    }

    window.onload = init;
  </script>

</body>
</html>
